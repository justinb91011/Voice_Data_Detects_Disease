{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Data Detects Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#wfdb is the package that can read WFDB format file\n",
    "import wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read the information text files first associated with all 208 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and process the information text files\n",
    "def read_info_file(file_path):\n",
    "  data = {}\n",
    "  with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "      # Split the line into key and value parts\n",
    "      if ':' in line:\n",
    "        parts = line.split(':')\n",
    "        if len(parts) >= 2:  # Check if there are at least two parts\n",
    "          key = parts[0].strip()\n",
    "          value = parts[1].strip()\n",
    "          if value == 'NU':  # Handle the 'Not Used' values\n",
    "            value = np.nan\n",
    "          elif value.replace(',', '.').replace(' gr', '').replace(' litres', '').isdigit():\n",
    "          # Convert numbers with commas as decimal points and remove units\n",
    "            value = float(value.replace(',', '.').replace(' gr', '').replace(' litres', ''))\n",
    "          elif value.isdigit():  # Convert digit strings to integers\n",
    "            value = int(value)\n",
    "            data[key] = value\n",
    "          elif '\\t' in line:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:  # Check if there are at least two parts\n",
    "              key = parts[0].strip()\n",
    "              value = parts[1].strip()\n",
    "              data[key] = value\n",
    "                \n",
    "  # Convert the dictionary into a DataFrame with a single row\n",
    "  info_df = pd.DataFrame([data])\n",
    "    \n",
    "  return info_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using wfdb we will read the .hea files associated with all 208 subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the .dat and .hea files\n",
    "def read_signal_files(dat_file_path, hea_file_path):\n",
    "    # Use wfdb to read the files and extract features\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a loop to start reading all the files for the 208 subjects within the databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Path: voice-database/voice001-info.txt\n"
     ]
    },
    {
     "ename": "NetFileNotFoundError",
     "evalue": "404 Error: Not Found for url: https://physionet.org/content/voice-database/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetFileNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m info_data \u001b[38;5;241m=\u001b[39m read_info_file(info_path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Read and process signal files\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m signal_data, sampling_frequency, signal_labels \u001b[38;5;241m=\u001b[39m read_signal_files(record_name, base_path)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Combine the info and signal data into one record\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# You will need to decide how to structure this combined data.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# This is just an example of combining the data into a dictionary.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m: info_data,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m: signal_data,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m: sampling_frequency,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: signal_labels\n\u001b[1;32m     31\u001b[0m }\n",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m, in \u001b[0;36mread_signal_files\u001b[0;34m(record_name, base_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_signal_files\u001b[39m(record_name, base_path):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Note: We expect record_name to be in the format 'voice001', 'voice002', etc.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Read the signal data from the record\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     record \u001b[38;5;241m=\u001b[39m wfdb\u001b[38;5;241m.\u001b[39mrdrecord(record_name, pn_dir\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Read the metadata from the header file associated with the record\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     header \u001b[38;5;241m=\u001b[39m wfdb\u001b[38;5;241m.\u001b[39mrdheader(record_name, pn_dir\u001b[38;5;241m=\u001b[39mbase_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wfdb/io/record.py:2026\u001b[0m, in \u001b[0;36mrdrecord\u001b[0;34m(record_name, sampfrom, sampto, channels, physical, pn_dir, m2s, smooth_frames, ignore_skew, return_res, force_channels, channel_names, warn_empty)\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (pn_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pn_dir):\n\u001b[1;32m   2024\u001b[0m     dir_list \u001b[38;5;241m=\u001b[39m pn_dir\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2025\u001b[0m     pn_dir \u001b[38;5;241m=\u001b[39m posixpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 2026\u001b[0m         dir_list[\u001b[38;5;241m0\u001b[39m], download\u001b[38;5;241m.\u001b[39mget_version(dir_list[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m*\u001b[39mdir_list[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m   2027\u001b[0m     )\n\u001b[1;32m   2029\u001b[0m record \u001b[38;5;241m=\u001b[39m rdheader(record_name, pn_dir\u001b[38;5;241m=\u001b[39mpn_dir, rd_segments\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;66;03m# Set defaults for sampto and channels input variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wfdb/io/download.py:238\u001b[0m, in \u001b[0;36mget_version\u001b[0;34m(pn_dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m url \u001b[38;5;241m=\u001b[39m posixpath\u001b[38;5;241m.\u001b[39mjoin(PN_CONTENT_URL, db_dir) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _url\u001b[38;5;241m.\u001b[39mopenurl(url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 238\u001b[0m     content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    239\u001b[0m contents \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m content\u001b[38;5;241m.\u001b[39msplitlines()]\n\u001b[1;32m    240\u001b[0m version_number \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m contents \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersion:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wfdb/io/_url.py:581\u001b[0m, in \u001b[0;36mNetFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid size: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (size,))\n\u001b[0;32m--> 581\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_range(start, end))\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(result)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wfdb/io/_url.py:474\u001b[0m, in \u001b[0;36mNetFile._read_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    471\u001b[0m         req_end \u001b[38;5;241m=\u001b[39m req_start \u001b[38;5;241m+\u001b[39m buffer_size\n\u001b[1;32m    472\u001b[0m         buffer_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RangeTransfer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_url, req_start, req_end) \u001b[38;5;28;01mas\u001b[39;00m xfer:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Update current file URL.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_url \u001b[38;5;241m=\u001b[39m xfer\u001b[38;5;241m.\u001b[39mresponse_url\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# If we requested a range but the server doesn't support\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# random access, then unless buffering is disabled, save\u001b[39;00m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# entire file in the buffer.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wfdb/io/_url.py:168\u001b[0m, in \u001b[0;36mRangeTransfer.__init__\u001b[0;34m(self, url, start, end)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39miter_content(\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_headers(method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wfdb/io/_url.py:214\u001b[0m, in \u001b[0;36mRangeTransfer._parse_headers\u001b[0;34m(self, method, response)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m NetFileError\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Error: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;241m%\u001b[39m (response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason, response\u001b[38;5;241m.\u001b[39murl),\n\u001b[1;32m    217\u001b[0m         url\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    218\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Parse the Content-Range if this is a partial response.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m206\u001b[39m, \u001b[38;5;241m416\u001b[39m):\n",
      "\u001b[0;31mNetFileNotFoundError\u001b[0m: 404 Error: Not Found for url: https://physionet.org/content/voice-database/"
     ]
    }
   ],
   "source": [
    "# Base path to the voice-database folder\n",
    "base_path = 'voice-database'\n",
    "\n",
    "# Initialize a list to hold all combined subject data\n",
    "all_subject_data = []\n",
    "\n",
    "# Loop over all subject IDs\n",
    "for subject_id in range(1, 209):  # Assuming IDs from 001 to 208\n",
    "    # Construct file paths\n",
    "    info_path = os.path.join(base_path, f'voice{subject_id:03}-info.txt')\n",
    "    dat_path = os.path.join(base_path, f'voice{subject_id:03}.dat')\n",
    "    hea_path = os.path.join(base_path, f'voice{subject_id:03}.hea')\n",
    "    txt_path = os.path.join(base_path, f'voice{subject_id:03}.txt')\n",
    "    \n",
    "    print(f\"Info Path: {info_path}\")\n",
    "    print(f\"Dat Path: {dat_path}\")\n",
    "    print(f\"Hea Path: {hea_path}\")\n",
    "    print(f\"Txt Path: {txt_path}\")\n",
    "    \n",
    "    # Read and process info file\n",
    "    info_data = read_info_file(info_path)\n",
    "    \n",
    "    # Read and process signal files (.dat and .hea)\n",
    "    signal_data = read_signal_files(dat_path, hea_path)\n",
    "    \n",
    "    # Combine the info and signal data into one record\n",
    "    combined_data = ...  # You need to decide how to combine these\n",
    "    \n",
    "    # Append the combined data to the list\n",
    "    all_subject_data.append(combined_data)\n",
    "\n",
    "# Optionally, convert the list to a DataFrame\n",
    "all_subjects_df = pd.DataFrame(all_subject_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
